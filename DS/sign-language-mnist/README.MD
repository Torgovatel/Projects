# Описание

**Входные данные:**  
Изображения 28x28 пикселей в градациях серого из [датасета](https://www.kaggle.com/datasets/datamunge/sign-language-mnist/data).  

**Активационная функция:**  
Для связи между слоями использовалась функция [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)), поскольку первостепенная задача каждой модели — получение [эмбеддинга](https://habr.com/ru/companies/wunderfund/articles/590651/), являющегося линейной комбинацией исходных признаков, значения которых находятся в отрезке $[0,255]$.  

**Обучение:**  
Достижение оптимальных весов происходит посредством оптимизации через [адаптивную оценку моментов](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA#Adam). Для приближения используется loss-функция [кросс-энтропии](https://translated.turbopages.org/proxy_u/en-ru.ru.da24c7ee-679e1856-687f88ba-74722d776562/https/en.wikipedia.org/wiki/Cross-entropy?), минимизируя расхождение истинного распределения классов с модельным.

# Используемые модели:

### 1. ***PerceptronClassifier:***
Представляет собой реализацию модели [многослойного персептрона](https://ru.wikipedia.org/wiki/%D0%9F%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD), с 1 скрытым слоем, который является линейной комбинацией входов, осуществляющей примитивное обобщение входного изображения в [эмбеддинг](https://habr.com/ru/companies/wunderfund/articles/590651/) заданного размера. Выходы же представляют собой линейную комбинацию полученных признаков.

**Преимущества модели:**  
- Простота и хорошая интерпретируемость.
- Высокая скорость обучения.  
- Малое количество параметров, что ускоряет обучение и уменьшает требования к вычислительным ресурсам. 

**Недостатки модели:**  
- Ограниченная способность к извлечению сложных признаков, особенно на изображениях с высокой размерностью.  
- Неэффективность на изображениях с множественными вариациями (повороты, масштабы, искажения).  
- Может страдать от переобучения при отсутствии регуляризации (при обучении использовалась $L_2$ регуляризация).

### 2. ***CNNClassifier:***
Представляет собой модель [сверточной нейронной сети](https://ru.wikipedia.org/wiki/%D0%A1%D0%B2%D1%91%D1%80%D1%82%D0%BE%D1%87%D0%BD%D0%B0%D1%8F_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C), следующей архитектуры:
- Сверточный слой #1: 1 канал $\rightarrow$ 32 фильтра, ядро 3×3, ReLU. Выявляет основные признаки.
- Макспулинг #1: снижение размера (28×28 $\rightarrow$ 14×14). Снижает вычислительную сложность и повышает устойчивость к сдвигам.
- Сверточный слой #2: 32 канала $\rightarrow$ 64 фильтра, ядро 3×3, ReLU. Выявляет весомые комбинации наиболее значимых основных признаков.
- Макспулинг #2: снижение размера (14×14 $\rightarrow$ 7×7). Отбрасывает незначимые комбинации.
- Полносвязный слой #1: 64×7×7 $\rightarrow$ 128, ReLU. Выравнивание признаков в итоговый [эмбеддинг](https://habr.com/ru/companies/wunderfund/articles/590651/) для изображения.
- Полносвязный слой #2: 128 $\rightarrow$ output_size (25). Классификация по полученному эмбеддингу.

**Преимущества модели:**  
- Способность извлекать сложные и многогранные признаки из изображений с помощью сверточных слоев.
- Устойчивость к изменениям положения объектов на изображении благодаря макспулингу.
- Эффективность на больших датасетах с высокими размерами изображений.

**Недостатки модели:**  
- Интерпретируемость не гарантирована.
- Большое количество параметров и, как следствие, потребность в больших вычислительных мощностях.
- Требует больше времени на обучение по сравнению с персептроном.

# Вывод 
Учитывая малый размер изображений сложность модели становится менее весмомым признаков при сравнении, от чего выбор сделан в пользу ***CNNClassifier*** т.к. точность на тестовых данных около 90% (на 20% больше чем у PerceptronClassifier).\
Разница в accuracy обусловлена тем, что архитектура CNN изначально создавалась для обработки изображений.